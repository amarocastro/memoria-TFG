\chapter{Estado de los recursos}
\label{chap:estadoInfraestructuraSistema}
\lettrine{C}{on} el fin de contextualizar los recursos que se utilizarán en este trabajo, en este capítulo se expone la situación actual de toda la infraestructura en lo relacionado al software que está en funcionamiento, a los recursos físicos de los que se compone, y al estado actual de las herramientas que rodean a dichos recursos.

\section{Infraestructura}
La infraestructura física de este servicio de virtualización, se encuentra localizada en el edificio del CITIC de la UDC, dentro de un rack alojado en su Centro de Proceso de Datos (CPD) \cite{citicUDC}. Está formado por 5 nodos \textit{Lenovo NeXtScale nx360 M5} y 3 nodos \textit{Dell EMC PowerEdge R740}. Ambos componentes dan flexibilidad en cuanto a la escalabilidad y ofrecen gran rendimiento de cómputo.\\

Especificaciones principales de los nodos:
\begin{itemize}
    \item Lenovo NeXtScale nx360 M5: 
        \begin{itemize}
            \item CPU: Dos Intel Xeon E5-2650
            \item Memoria: 128 GB
            \item Tarjeta  gráfica: Tesla M60
        \end{itemize}
            Más información: \url{https://lenovopress.com/tips1195-nextscale-nx360-m5-e5-2600-v3}
    \item Dell EMC PowerEdge R740:
        \begin{itemize}
            \item CPU: Dos Xeon Gold 6146
            \item Memoria: 384 GB
            \item Tarjeta gráfica: Tesla P40
        \end{itemize}
    Más información: \url{https://www.dell.com/es-es/work/shop/servidores-almacenamiento-y-redes/smart-value-poweredge-r740-server-standard/spd/poweredge-r740/per7400m}
\end{itemize}
El almacenamiento está colocado físicamente en la misma ubicación que los hosts pero en su abstracción lógica este es independiente y está separado de cada nodo. Está conformado por 13 discos duros SSD de 3.84 TB de capacidad, obteniendo así una capacidad total de casi 50 TB, pero que utilizan la configuración de almacenamiento RAID 5[Pal. \ref{itm:raid5}] lo cual permite conseguir mayor  integridad de los datos, tolerancia a fallos y ancho de banda, reduciendo la cantidad de almacenamiento utilizable a 34 TB. Estos discos forman un \textit{pool} de almacenamiento que se divide en cinco LUNs (\textit{Logical Storage Unit})[Pal. \ref{itm:lun}] de 2 TB cada una, representadas en el sistema de virtualización como cinco \textit{datastores} diferentes que utilizan el sistema de archivos VMFS el cual optimiza el almacenamiento de máquinas virtuales.\\
Los discos duros físicos están colocados en una misma cabina y son accesibles por todos los nodos a través de dos switches para aportar redundancia. Para ello, las cabinas incorporan dos controladores con conexión SFP+[Pal. \ref{itm:sfp}] que se conectan a cada switch mediante dos puertos que aportan conectividad 10 Gb y, además, incorporan otros dos puertos con conectividad 1 Gb para la gestión de los discos. Estas conexiones utilizan los protocolos de red Ethernet y iSCSI, formando así, junto con el resto de componentes descritos, la estructura de una SAN [\ref{fig:esquemaentornoreal}].\\

La gestión del almacenamiento se realiza en la capa física, el nivel más bajo por lo que la configuración de cada LUN que utilizan las máquinas virtuales desplegadas se tiene que hacer antes de conocer los requisitos necesarios de lo que se vaya a desplegar en la capa software. Esto provoca que si se quiere desplegar una máquina virtual con más capacidad de almacenamiento o con una estructura RAID diferente haya que crear una nueva LUN que se adapte a los requisitos.
Esta gestión hace que el uso de recursos de almacenamiento no sea el óptimo ya que no permite ajustar de forma precisa y rápida cada configuración a los requisitos necesarios generando así mayor coste.

\iffalse
Los nodos acceden a los discos de almacenamiento a través de dos switches que al mismo tiempo dan conectividad entre los nodos formando una SAN. Los 10TB de capacidad están repartidos entre cinco DataStores, de 2TB cada uno. Una máquina virtual está alojada en un DataStore concreto pero puede tener ficheros alojados en varios almacenes de datos diferentes.
Las conexiones entre dispositivos son Ethernet 10 Gigabit combinado con el protocolo de transporte ISCSI
\fi

\section{Software}
\label{subsec:softwareinstalado}
Actualmente el servicio está basado en el software de la empresa VMware, uno de los principales proveedores de software de virtualización, siendo \textbf{VMware vSphere} el software desplegado sobre la infraestructura. Este producto de VMware es el encargado de virtualizar parte de la infraestructura física y de proporcionar las herramientas necesarias para gestionarla. Sus \underline{principales componentes} son los siguientes:
\begin{itemize}
    
    \item \textbf{ESXi}: Hipervisor propio de VMware, de tipo 1 o \textit{bare metal}[Pal. \ref{itm:baremetal}]. No requiere de sistema operativo para funcionar ya que funciona directamente sobre el hardware físico\cite{Esxi}. Este hipervisor está instalado en cada uno de los ocho nodos que forman la infraestructura.
    
    \item \textbf{VMware vCenter Server}: servicio que actúa como un administrador central para todas las máquinas virtuales y hosts. Normalmente, los servicios descritos en este apartado están disponibles para una instancia de vCenter Server agrupados en un \textit{Platform Services Controller} (PSC) [Pal. \ref{itm:psc}]
    
    \item \textbf{vCenter Single Sign-On}: es un servicio de autenticación. Permite que los usuarios solo se tengan que autenticar una vez cuando acceden al entorno de la infraestructura a través de vSphere Client, en lugar de tener que autenticarse varias veces en cada componente. Cuando el usuario se autentica por primera vez, este recibe un token que le permitirá autenticarse en el resto de componentes sin volver a introducir sus credenciales. También se encarga de la administración perfiles de usuarios y de los dominios de autenticación (esto permite usar directorios de usuarios externos).\label{itm:singlesingonEX}
    
    \iffalse
    \item \textbf{vSphere License Server}: permite gestionar e inventariar licencias para aquellos sistemas conectados a un \emph{Platform Services Controller}.
    
    \item \textbf{VMware Certificate Authority}: provee con un certificado a cada nodo. está firmado por esta misma autoridad (VMCA).
    \fi
    \iffalse
    \item \textbf{PostgreSQL}: distribución de la base de datos PostgreSQL para vSphere.
    \fi
    \item \textbf{vSphere Web Client} y \textbf{vSphere Client}: interfaz que permite conectarse a una instancia de vCenter Server para gestionar la infraestructura.
    
    
    \iffalse
    \item \textbf{vSphere ESXi Dump Collector}: permite configurar un host ESXi para que guarde su memoria en un servidor externo en lugar de en un disco cuando hay algún fallo crítico.
    \item \textbf{vSphere Syslog Collector}: habilita logs de red. 
    \fi
    \item \textbf{vSphere Auto Deploy}: herramienta que permite desplegar gran cantidad de nodos físicos de forma automátizada.
    
    \item \textbf{vSphere Update Manager}: permite gestionar de forma centralizada y automatizada las actualizaciones para hosts ESXi, para el hardware de las máquinas virtuales y para actualizar e instalar software de terceros en los hosts ESXi. Este componente se ejecuta desde VMware vCenter Server como un servicio y requiere conexión a la red externa para obtener las actualizaciones.
   
    \item \textbf{vSphere Web Client}: interfaz web que permite acceder a vCenter Server de forma remota.
    
    \item \textbf{vMotion}: permite la migración de máquinas virtuales de un host a otro de forma transparente y sin detener su ejecución. Permite planificar las migraciones de máquinas virtuales entre hosts que pueden pertenecer a distintos clusters.
    
    \item \textbf{Storage vMotion}: permite migrar los discos y configuración de una máquina virtual de un \textit{datastore} a otro sin interrumpir el servicio.
    
    \item \textbf{vSphere High Availability (HA)}: provee alta disponibilidad para las máquinas virtuales. En caso de que una máquina virtual deje de estar activa, este servicio intenta encender la máquina  virtual en otro host dentro del mismo cluster automáticamente. Este solo actúa en caso de fallo de un host, mientras que vMotion solo actúa cuando las migraciones se hacen entre hosts activos. Proporciona escalabilidad gracias a su modelo Maestro - Esclavo, fiabilidad gracias a que no tiene dependencias con otros servicios y a que se puede comunicar con las máquinas a través de varios caminos, y usabilidad gracias a que tiene una interfaz sencilla.
    
    \item \textbf{vSphere Distributed Resource Scheduler (DRS)} y \textbf{vSphere Distributed Power Management (DPM)}: DRS genera recomendaciones sobre donde se debería desplegar una máquina virtual cuando se está creando, utiliza vMotion para mover las máquinas virtuales a través de los hosts de un cluster para maximizar el rendimiento o durante tareas de mantenimiento en un host. DPM se encarga de gestionar el consumo de energía de cada host según el rendimiento necesario.
    \item \textbf{Storage DRS}: balancea la carga de almacenamiento y las operaciones I/O entre los diferentes datastores disponibles.
    
    \item \textbf{vSphere Fault Tolerance}: crea una copia de todos los archivos y discos de cada máquina virtual sincronizados con los originales. Esto junto con vSphere HA y vSphere DRS, proporciona recuperación ante fallos y  disponibilidad continua de las máquinas virtuales de forma automática en caso de que la máquina virtual esté inactiva, sin perdida de datos y sin pérdida de conexiones de que había activas. Este servicio está orientado a proteger aquellas tareas que requieren un alto rendimiento o que son críticas.
    \iffalse
    habilitadas creando una copia de cada una  para usarla en caso de que la primera falle.
    \fi
    \item \textbf{vSphere Distributed Switch (vDS)}: habilita switches virtuales que se encargan de gestionar el tráfico de los hosts ESXi. Se utiliza para administrar la configuración de los puertos de cada host ESXi de forma centralizada, permitiendo crear redes y establecer políticas sobre el tráfico desde un único lugar. Desde aquí se establecen las conexiones de cada tipo de tráfico con las tarjetas de red físicas de cada host ESXi.
    
    \item \textbf{Virtual Machine File System (VMFS)}: sistema de archivos de alto rendimiento nativo de VMware vSphere. Se utiliza para implementar los almacenes de datos de la infraestructura y está optimizado para el almacenamiento de máquinas virtuales.
    
\end{itemize}

\begin{figure}[hp]
  \centering
  \includegraphics[width=0.75\textwidth]{imaxes/cap2recursos/contentVSphere}
  \caption{Elementos de la plataforma VMWare vSphere\cite{fotovSphere}}
  \label{fig:componentesVSphere}
\end{figure}
\begin{figure}[hp]
  \centering
  \includegraphics[width=1\textwidth]{imaxes/cap2recursos/recursosReal.png}
  \caption{Esquema de los recuros software y hardware del entorno}
  \label{fig:esquemaentornoreal}
\end{figure}

\FloatBarrier
\subsection{Estado de la tecnología}

En los últimos tiempos los servicios de \textit{Infrastructure as a Service} (IaaS) se han extendido de forma considerable con la aparición de software que permite la gestión de un sistema de Cloud Computing, como pueden ser VMware Cloud Foundation (2011), OpenStack (2010), o Apache CloudStack (2012). Estas herramientas construyen una infraestructura virtual sobre un entorno físico estandarizado que les permite administrar y automatizar la escalabilidad, el sistema de almacenamiento, la disponibilidad del servicio, la red, y la seguridad del servicio, con lo que se consigue reducir el coste y el tiempo de gestión y configuración, mejorando la eficiencia de infraestructura física.

A la hora de alcanzar los objetivos descritos en este proyecto se nos plantea la duda de que solución software desplegar ya que actualmente existen tres principales alternativas en el mercado, VMware Cloud Foundation, OpenStack y Apache CloudStack. Cada una de ellas ofrece diferentes características con diferentes requisitos que se pueden adaptar mejor o peor al entorno de despliegue, pero después de comprobar esos aspectos tenemos claro que la \underline{solución elegida es VMware Cloud Foundation}.

\subsubsection{VMware Cloud Foundation}
Esta solución virtualiza todas las capas de la infraestructura[Fig. \ref{fig:infraCloudFound}] (red, computación y almacenamiento) combinando cuatro componentes principales, vSphere para gestionar el cómputo, vSAN para la gestión del almacenamiento, NSX para la gestión de la red, y vRealize para gestionar todas las operaciones que tienen lugar en el servicio, integrando todos los componentes para que la gestión de la infraestructura sea lo más simple posible. Este cojunto de herramientas convierten el CPD en un \textit{Software Defined Datacenter} (SDDC), un entorno donde todas las partes físicas de la infraestructura pasan a estar controladas a través de software haciendo más flexible, independiente y menos costosa la configuración de estos componentes. Las \underline{principales características} de VMware Cloud Foundation son:
\begin{itemize}
    \item \textbf{Servicios software con integración nativa}: ofrece un conjunto de servicios software para el almacenamiento, red, seguridad y gestión de la cloud. Estos servicios se integran de forma nativa con la infraestructura minimizando las tareas de configuración y administración.
    \item \textbf{Escalabilidad y elasticidad de los recursos}: la capacidad de la infraestructura se puede modificar de forma sencilla gracias a la automatización del ciclo de vida de todos los elementos. 
    \item \textbf{Supervisión de los recursos}: ofrece supervisión de los recursos con reconocimiento de aplicaciones y solución de problemas, permitiendo conocer todos los eventos que tienen lugar en la infraestructura.
    \item \textbf{Aprovisionamiento automatizado}: todos los componentes necesarios para formar un SDDC son desplegados automáticamente por VMware Cloud Foundation, incluyendo los recursos informáticos, los componentes de almacenamiento, los de red y los de administración.
    \item \textbf{Ciclo de vida automatizado}: automatiza las operaciones previas, iniciales y posterios de una plataforma de software para ofrecer una gestión más sencilla. Esto incluye su implementación, el aprovisionamiento de clústeres aislados bajo demanda y la instalación de actualizaciones y parches.
   
   \iffalse 
    \item \textbf{Experiencia de usuario simple}: gracias a la gran cantidad de procesos automatizados.
    \item \textbf{Escalabilidad modular}: el sistema y la infraestructura se puede escalar de forma sencilla.
    \item \textbf{Cloud híbrida}: da la posibilidad de conectar una Cloud pública con una Cloud privada y así tratar ambas como una única Cloud.
    \fi
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{imaxes/cap2recursos/SDDCoverview.png}
  \caption{Partes virtualizadas en un SDDC.}
  \label{fig:sddcoverview}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{imaxes/cap2recursos/overviewCF.png}
  \caption{Cloud Foundation virtualiza toda la infraestructura.}
  \label{fig:infraCloudFound}
\end{figure}
\FloatBarrier

VMware Cloud Foundation permite reducir el tiempo de mantenimiento ya que todo está controlado por el software que integra todos los componentes, automatizando gran parte de las operaciones y el ciclo de vida de todos los elementos desde su creación, como puede ser el control de versiones de cada elemento, los perfiles de usuario y las máquinas virtuales creadas, además de proporcionar una plataforma de acceso para que cada usuario pueda gestionar sus recursos. Además, su arquitectura se divide en entornos aislados administrados bajo demanda desde un entorno principal. Cada uno de esos entornos tiene un conjunto de recursos dedicados cuyas capacidades se pueden ajustar para proporcionar determinadas características, como por ejemplo alta disponibilidad, mayor capacidad de almacenamiento o mayor capacidad de cómputo. 

\iffalse
*****************************************************\\
También permite separar las cargas de trabajo mediante Dominios según el tipo de trabajo que se va a realizar, pudiendo acceder a cada uno de ellos de forma separada.\\
*****************************************************\\
\fi

Para poder usar este software es necesaria la adquisición de licencias, estas se organizan por componente y por número de hosts sobre los que se va a instalar el producto. A pesar de tener un coste elevado, teniendo en cuenta los beneficios que aporta en cuanto integración nativa con los componentes ya instalados y a que su mantenimiento es más sencillo, se ha elegido este paquete ya que es más rentable que otras opciones. \\
Si bien VMware ofrece plugins para conectar sus componentes con otras soluciones, como es el caso de OpenStack \cite{opestackintegrated}, estos no ofrecen el rendimiento que da la integración nativa, además, en caso de recibir actualizaciones, habría que actualizar cada componente de forma individual aumentando el riesgo de incompatibilidades con el resto de elementos del sistema, mientras que Cloud Foundation gestiona todo el ciclo de vida de cada actualización para cada componente, permitiendo comprobar si existe alguna incompatibilidad con el resto de versiones antes de aplicar una actualización. En definitiva, VmWare Cloud Foundation simplifica el proceso de instalación, configuración, gestión, y mantenimiento, tanto para los usuarios como para el administrador del sistema.

\subsubsection{Componentes de VMware Cloud Foundation \cite{componentesCloudFound}}
\label{subsubsect:cfcomponents}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{imaxes/cap2recursos/SDDCoverview.png}
  \caption{Partes virtualizadas en un SDDC.}
  \label{fig:sddcoverview}
\end{figure}
\FloatBarrier

El SDDC de VMware Cloud Foundation está dividido en varias capas y módulos con las cuales se gestiona todas las partes de la infraestructura [Fig. \ref{fig:sddcoverview}]. Cada una de estas capas incluye una serie de productos y servicios, con unas funciones concretas que permiten gestionar un entorno complejo de una forma más sencilla. Las partes de esta infraestructura son las siguientes:
\begin{itemize}
    \item \textbf{Capa de Infraestructura física}: Es la base de la infraestructura y en ella residen los componentes físicos de almacenamiento, red y cómputo.
    \item \textbf{Capa de Infraestructura Virtual}: Se encuentra sobre la capa física y se encarga de realizar las tareas de acceso a los recursos físicos para controlar su aprovisionamiento.
    \item \textbf{Capa de Gestión Cloud}: Es la capa superior y se dedica a las tareas de consumo de los recursos, es decir, realiza peticiones a las capas inferiores para la obtención de los recursos.
    \item \textbf{Capa de Gestión de Servicios}: Esta capa se centra en el mantenimiento de la arquitectura gestionando el ciclo de vida, la monitorización, logs y alertas de los componentes. 
    \item \textbf{Capa de Gestión de Operaciones}: La función de esta capa es la monitorización de la capa de infraestructura física, de la capa de infraestructura virtual y de los flujos de trabajo de los usuarios.
    \item \textbf{Capa de Continuidad del Servicio}: Contiene elementos que ayudan a que el servicio se mantenga activo y a evitar la pérdida de información crítica, proveyendo copias de seguridad, restauración y recuperación ante desastres.
    \item \textbf{Capa de Seguridad}: Se encarga de que todos los componentes de la infraestructura estén bien delimitados para establecer un nivel de seguridad.
\end{itemize}

En VMware Cloud Foundation existen diversos productos y servicios, algunos son requeridos para construir la instancia mínima de un SDDC y otros ofrecen servicios adicionales que incrementan el rendimiento de la infraestructura. A continuación se describen aquellos componentes necesarios para la instalación mínima de VMware Cloud Foundation y que se usarán en la implementación de este proyecto:

\begin{itemize}
    \item \textbf{SDDC Manager}: gestiona el ciclo de vida de todos los componentes del sistema, incluyendo el proceso inicial de despliegue de Cloud Foundation, su configuración y aprovisionamiento, y las actualizaciones. Monitoriza los recursos físicos y lógicos de la infraestructura, facilita su configuración y permite añadir nuevos recursos cuando sea necesario. \iffalse Todo esto lo realiza mediante flujos de trabajo que facilitan la detección de orígenes de errores.\fi
    \item \textbf{vSphere}: ya está incluído en el servicio actual [\ref{subsec:softwareinstalado}].
    \item \textbf{VMware vSAN}: componente clave que virtualiza el almacenamiento. Como ya se ha explicado, el almacenamiento del servicio actual está configurado con LUNs que se deben gestionar individualmente en una capa distinta a los componentes software, provocando que su configuración sea más compleja y costosa. El objetivo de VMware vSAN es gestionar de forma automatizada y desde un único lugar el sistema de almacenamiento de la infraestructura, tratando toda la capacidad y recursos de almacenamiento como un único elemento, eliminando así la necesidad de tener que crear LUNs aisladas, consiguiendo abstraer la configuración de almacenamiento de la capa física en la capa de software y permitiendo establecer políticas de almacenamiento desde cada máquina virtual para adecuarlo a las necesidades de cada una, sin tener que editar la configuración real del entorno físico. Así el rendimiento de los recursos de almacenamiento es más eficiente, flexible y su configuración se integra dentro del mismo servicio junto con la gestión del resto de componentes. \\
    En VMware vSAN, en lugar de tratar el almacenamiento de forma independiente este pasa a estar ligado a cada host, es decir, cada uno de los nodos tiene asignados hasta cinco grupos de discos. Estos grupos de discos pueden ser de tipo \textit{Hybrid}, donde se combinan discos duros SSD y HDD, o \textit{All-Flash}, donde todos los discos son de tipo SSD. Dentro de cada grupo, los discos se dividen en dos tipos con distintas funciones, el disco de caché y el disco de capacidad\cite{operacionesVSAN}:
        \begin{itemize}
            \item \textbf{Caché}: Hay uno en cada grupo. Realiza la función de memoria caché y se encarga de escribir los datos persistentes en los discos de capacidad.
            \item \textbf{Capacidad}: Puede haber hasta siete discos en cada grupo. Almacena los datos persistentes del entorno.
        \end{itemize}
    En cada grupo de discos la gestión de la \underline{lectura y escritura} de datos se hace de la siguiente forma:
        \begin{itemize}
            \item \textbf{Lectura}: En el caso de la solución \textit{Hybrid}, si el dato que se busca no está en el disco de caché entonces se busca en los discos de capacidad y después se incorpora al disco de caché. Con la solución \textit{All-flash}, los datos se leen siempre directamente de los discos de capacidad sin que estos sean escritos en el disco de caché dejando a este completamente libre para las operaciones de escritura. Gracias a esto, la estructura \textit{All-flash} ofrece mayor rendimiento respecto a la \textit{Hybrid}.
            \item \textbf{Escritura}: Tanto en la solución \textit{Hybrid} con en la \textit{All-flash}, el host ESXi primero escribe en el disco de caché, este le responde con una confirmación de escritura y más tarde vSAN se encarga de escribir ese dato en los discos de capacidad cuando el disco de caché está casi completo o cuando el dato lleva un tiempo sin ser utilizado.
        \end{itemize}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=1\textwidth]{imaxes/cap2recursos/rendimientoVSAN.png}
            \caption{Almacenamiento All-Flash vs. Híbrido en vSAN}
            \label{fig:rendimientoVSAN}
        \end{figure}
        \FloatBarrier
    Los hosts acceden al \textit{datastore} de vSAN mediante protocolo IP en una red accesible por todos los nodos. \\ 
    Este componente permite reducir las tareas de gestión del almacenamiento físico ya que ya no es necesario hacer ajustes en la capa física para cumplir unos requisitos en la capa software.
   
    \item \textbf{VMware NSX-T}: otro de los componentes clave. Tiene un papel similar a vSAN, pero en este caso se encarga de virtualización de los componentes físicos de la red de la infraestructura, es decir, abstrae los componentes de la red para desacoplar la configuración de la red física y la red virtual del SDDC simplificando así las operaciones en la infraestructura física. Con esto, los administradores pueden configurar la red del SDDC sin tener en cuenta el equipamiento físico ya que trabajan con una red virtual independiente de la red física sobre la que funciona.
    \iffalse
    Incluye servicios de red como firewall, elimina el tráfico  DNS, DHCP, VPN, NAT, enrutamiento, balanceo de carga, o switching, que permiten reducir la cantidad de dispositivos físicos de red.\\
    \fi
    Los componentes de VMware NSX-T se agrupan en tres capas/planos\cite{componentesNSX}:
    \begin{itemize}
        \item \textbf{Management Plane}: desde este punto se gestiona la conectividad, seguridad y operaciones de los componentes del sistema a través de un inventario de objetos, proporciona una interfaz y una API para acceder acceder a los componentes de VMware NSX-T, almacena y transmite la configuración del sistema al \textit{Control Plane} y obtiene información del sistema como estadísticas de uso. Estas funciones son implementadas por el componente \textbf{NSX-T Manager}.
            
            % \begin{itemize}
            %     \item \textbf{NSX-T Manager}: es el punto desde donde se administran todos los componentes de NSX. Existe una instancia de NSX Manager por cada instancia de vCenter Server. Contiene los archivos necesarios para desplegar el resto de componentes de NSX (NSX Edge Services Gateway y NSX Controller), y, cuando es creado, se encarga de habilitar las funcionalidades VXLAN sobre capa 2, Distributed Router y \textit{User World Agent} (UWA) en todos los hosts ESXi. Genera certificados para cada instancia de NSX Controller y host ESXi que utiliza para asegurar las comunicaciones en el \textit{Control Plane}.
            % \end{itemize}
        \item \textbf{Control Plane}: este plano comunica al cada componente la configuración establecida desde \textit{Management Plane}. Además, se encarga de obtener información sobre la topología de la red desde el \textit{Data Plane}. Este plano se divide en dos partes:
            \begin{itemize}
                \item \textit{Central Control Plane} (CCP): implementado por un cluster de varias VM donde cada una contiene el componente \textbf{NSX-T Controller}, el cual es el encargado de controlar las redes virtuales existentes y de proveer de configuración a otros componentes como \textit{logical switches}, \textit{logical routers} y \textit{edge nodes}. El componente \textbf{NSX-T Controller} está integrado, junto con el componente \textbf{NSX-T Manager}, en una única \textit{appliance} denominada \textbf{NSX-T Manager Appliance}
                \item \textit{Local Control Plane} (LCP): este plano lo implementan los \textbf{transport nodes} y es responsable de monitorizar los enlaces locales y de reenviar entradas de configuración a los componentes del \textit{Data Plane}.
            \end{itemize}
            % \begin{itemize}
            %     \item \textbf{NSX Controller}: se encarga de almacenar y transmitir información sobre la red a cada hosts ESXi. Provee la información sobre los segmentos VXLAN existentes, la información sobre el enrutamiento en la red virtual, almacena las tablas ARP, MAC y VTEP. Esto permite que se pueda eliminar el tráfico Broadcast de las redes VXLAN, ya que este tipo de tráfico es generado por las máquinas virtuales y capturado por el host ESXi que lo reenvía a la instancia de NSX Controller para obtener la información requerida. También elimina la necesidad de configurar los protocolos Multicast Routing y \textit{Protocol Independent Multicast} (PIM) para implementar redes VXLAN. Se despliegan tres instancias de NSX Controller para proveer alta disponibilidad.
            %     \iffalse
            %     \item \textit{User World Agent} (UWA): proceso que usa una instancia de NSX Controller para comunicarse con cada host ESXi a través de su dirección IP en la red
            %     \fi
            %     \item \textbf{Logical Router Control}: se trata de una máquina virtual que se genera cada vez que se crea el componente Distributed Logical Router. Busca routers adyacentes, tanto físcos como virtuales, para formar tablas de enrutamiento que se envían a NSX Manager que las comunica al resto de dispostivos.
            % \end{itemize}
        \item \textbf{Data Plane}: este plano es responsable de retransmitir el tráfico por la red basándose en lass tablas  y reglas generadas por el \textit{Control Plane}. También informa al \textit{Control Plane} sobre la topología de la red, recopila estadísticas a nivel de paquete y controla el estado de los enlaces para resolver posibles fallos. En definitiva, el \textit{Data Plane} se centra en como gestionar los paquetes que circulan por la red. Está implementado por dos tipos de \textbf{transport nodes} (TN):
        \begin{itemize}
            \item \textit{Hypervisor Transpor Node}: son los hosts con el hipervisor ESXi y que utilizan VMware NSX-T para proveer servicios de red a las VM que funcionan sobre sobre el host.
            \item \textit{VMware NSX-T Edge Node}: también llamado \textit{Edge Node}, se trata de una \textit{appliance} instalado sobre un servidor físico o en forma de VM, que provee al entorno de una serie de servicios de red centralizados.
        \end{itemize}
VMware NSX-T inlcuye otros componentes:
    \begin{itemize}
        \item \textbf{NSX-T Virtual Distributed Switch} (N-VDS): funciona sobre el hipervisor ESXi de cada host y es el encargado de retransmitir el tráfico entre las VM que se encuentran dentro de un mismo \textit{transport node} y desde una VM a la red física de la infraestructura.
        \item \textbf{Logical Router}: Integra dos componentes, \textbf{Distributed Router} (DR) proporciona conectividad entre las redes virtuales que se crean dentro del SDDC funciona de forma distribuída en los \textit{transport nodes}), y \textbf{Service Router} (SR) da conectividad hacia redes redes externas al SSDC y ofrece una serie de servicios centralizados, NAT y DHCP entre otros.
        \item \textbf{Logical Firewall}: NSX-T permite establecer reglas para gestionar el tráfico de entrada y de salida del SDDC. Estas reglas se pueden establecer a nivel de \textit{Layer 2} o \textit{Layer 3} de la red.
        \item \textbf{Logical Load Balancer}: se encarga de distribuir el tráfico que recibe a los componentes correspondientes, permitiendo desacoplar el acceso a un servicio de su implementación. Esto permite que un servicio se pueda escalar y ofrecer alta disponibilidad.
    \end{itemize}
        
\FloatBarrier
        % \item \textbf{Data Plane}: está controlado por NSX Virtual Switch el cual extiende las funcionalidades de vSphere Distributed Router, clave para la virtualización de la red. Estas características son:
        %     \begin{itemize}
        %         \item \textbf{Distributed Firewall}: filtra el tráfico con un servicio de Firewall, permitiendo segmentar las máquinas virtuales estableciendo políticas según sus características. Está distribuído en todos los hosts ESXi.
        %         \item \textbf{VXLAN}: protocolo que habilita la creación de redes virtuales sobre una red IP ya existente. Es gestionado a través de \textit{VXLAN Tunnel EndPoints} (VTEP) repartidos por cada host ESXi para encapsular el tráfico saliente de las máquinas virtuales con la cabecera de la red VXLAN.
        %         \item \textbf{Distributed Logical Router} (DLR): elemento que reside en el hipervisor de cada host ESXi y que se encarga de enrutar el tráfico entre las máquinas virtuales a través de la red virtual del SDDC sobre VXLAN. Soporta los protocolos de enrutamiento dinámico BGP y OSPF. DLR pasa a llamarse Universal Distributed Logical Router (UDLR) cuando se configura el mismo UDLR a través de varias \textit{regions} de un SDDC.
        %         \item \textbf{Logical Switch}: se definen dentro del componente vSphere Distributed Switch sobre un \textit{portgroup}. Crea segmentos de red ya que tiene una VXLAN asignada, es decir, un segmento/red VXLAN está delimitada por un Logical Switch. Está distribuido en cada host ESXi\footnote{El Logical Switch se extiende en toda la \textit{transport zone} de la VXLAN a la que pertenece, esta define el alcance de un segmento VXLAN que puede extenderse en varios clusters pertenecientes a una instancia de vCenter Server.}.
        %         \item \textbf{Edge Services Gateway} (ESG): es una máquina virtual que se encarga de enrutar el tráfico hacia redes externas al SDDC, por lo tanto está conectado con el DLR y con el dispositivo físico de enrutamiento de capa 3. Provee servicios perimetrales como SSL, NAT, DHCP, VPN y Load Balancing.
        %     \end{itemize}
    \end{itemize}
    
        % \begin{figure}[h!]
        %     \centering
        %     \includegraphics[width=0.55\textwidth]{imaxes/conceptosPrevios/planosNSX.png}
        %     \caption{Componentes de VMware NSX}
        %     \label{fig:planosNSX}
        % \end{figure}

        
    % \iffalse
    %     \item \textbf{NSX Manager}: es el punto de entrada de la comunicaión con la instancia de vSphere, se encarga de desplegar NSX Controllers y el resto de servicios de NSX, instala VXLAN en los hosts ESXi, \textit{distributed routers} y firewalls, se comunica con los NSX Controllers y genera certificados para que las comunicaciones con los hosts ESXi sean seguras.
    %     \item \textbf{NSX Controller}: establece el punto de control desde donde se distribuye la información sobre el enrutamiento lógico y de VXLAN hacia los hosts ESXi, existen varios nodos de este tipo para facilitar el escalado y alta disponibilidad de la infraestructura, comparte la información de red entre el resto de NSX Controllers, abstrae el uso de VXLAN sobre la red física y elimina el uso del protocolo ARP en redes VXLAN. 
    %     \item \textbf{NSX Virtual Switch}: gestiona los vSphere vSwitch Distibuted desplegados, proporcionando switching entre los hosts ESXi.
    %     \item \textbf{NSX Logical Router Control}: se despliega cuando se crea un Distributed Logical Router. Se encarga de buscar adyacencias para generar tablas de enrutamiento que envía a NSX Manager y a los NSX Cotrollers para que informen a cada Distributed Logical Router en cada nodo ESXi.
    %     \item \textbf{NSX Edge}: proporciona servicios múltiples servicios como firewall perimetral de capa 2 y 3, SSL, NAT, DHCP, VPN, balanceo de carga y alta disponibilidad.
    % \fi
    % \item \textbf{VMware vRealize Log Insight}: realiza la gestión de logs del servicio, dando visibilidad a todas las operaciones del servicio, generando análisis del sistema. Esto permite tener mayor conocimiento de los riesgos, eficiencia y uso de recursos, a parte de facilitar la búsqueda de orígenes de errores. Está formado por los siguientes componentes:
    % \begin{itemize}
    %     \item \textbf{Nodo Master}: es el único nodo que se despliega en el modo \textit{standalone}, en ese caso se encarga de la obtención de logs de los servicios de VMware Cloud Foundation y de entregar la información almacenada al cliente. Cuando se despliegan más nodos, se encarga de gestionar la creación, elimación y actualización de los nodos worker.
        
    %     \item \textbf{Nodo Worker}: es un nodo opcional que se crea para proporcionar alta disponibilidad y escalar el entorno. En este tipo de nodo se delegan la entrega de la información almacena al cliente y la obtención de logs de cada host ESXi.
        
    %     \item \textbf{Load Balancer}: simplifica la configuración de la alta disponibilidad cuando hay nodos worker desplegados. Se encarga de recibir las peticiones de los clientes, de entregarlas a los nodos worker y de enviar la respueste. También gestiona el balanceo del tráfico entrante entre todos los nodos y de escoger el nodo \textit{master}. Se ejecuta en uno de los nodos de VMware vRealize Log Insight.
    % \end{itemize}
    
    \item \textbf{VMware vRealize Automation}: se trata de un componente opcional que permite automatizar el despliegue de máquinas virtuales, procesos y aplicaciones reduciendo la complejidad y eliminando tareas manuales. Esto acelera la entrega del servicio gracias a que las tareas de aprovisionamiento y entrega de recursos y aplicaciones son más rápidas permitiendo establecer políticas de seguridad y control. Internamente, este servicio \underline{cuenta con los siguientes componentes \cite{vRealizeAutomation}} para cumplir con sus funciones:
    \begin{itemize}
        \item \textbf{vRealize Automation Appliance}: contiene un portal donde los usuarios pueden acceder y gestionar y aprovisionar servicios cloud bajo demanda, un servicio de autenticación y una interfaz para gestionar este componente. 
        \item \textbf{IaaS Web Server}: realiza las tareas de administración de la infraestructura que se requieren desde el portal de vRealize Automation Appliance.
        \item \textbf{Microsoft SQL Server}: se utiliza para almacenar información sobre los elementos de IaaS y sobre las máquinas virtuales controladas por vRealize Automation Appliance.
    \end{itemize}
\end{itemize}

**Decir como se puede implementar un sistema de facturación.*******\\
**Como se puede conectar los usuarios de la UDC.********












\iffalse
\subsubsection{OpenStack}
Es una plataforma constituída por tres proyectos, uno dedicado a la gestión de la red, otro a la gestión del cómputo, y otro a la gestión del almacenamiento. Para poder desplegarlo sobre los componentes de VmWare instalados en nuestra infraestructura son necesarios tres plugins específicos para el software de VmWare. OpenStack aporta una API a través de la cual es posible gestionar los recursos virtuales y el aproivisionamiento.

\subsubsection{Apache CloudStack}
\fi


